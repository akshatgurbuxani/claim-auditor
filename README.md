# Claim Auditor

**An automated earnings verification system that extracts quantitative claims from earnings call transcripts, verifies them against SEC financial data, and detects patterns of misleading communication.**

Executives say things on earnings calls. This system checks if what they say is trueâ€”and whether it's intentionally misleading.

---

## Table of Contents

1. [Problem & Solution](#problem--solution)
2. [Key Features](#key-features)
3. [Quick Start (5 Minutes)](#quick-start-5-minutes)
4. [Production-Ready Architecture](#production-ready-architecture)
5. [Project Structure](#project-structure)
6. [How the Pipeline Works](#how-the-pipeline-works)
7. [Running the Pipeline](#running-the-pipeline)
8. [Viewing Results (Streamlit UI)](#viewing-results-streamlit-ui)
9. [Running Tests](#running-tests)
10. [Key Architectural Decisions](#key-architectural-decisions)
11. [API Clients & External Dependencies](#api-clients--external-dependencies)
12. [Verification Algorithm](#verification-algorithm)
13. [Cross-Quarter Pattern Detection](#cross-quarter-pattern-detection)
14. [MCP Server (AI Agent Integration)](#mcp-server-ai-agent-integration)
15. [Deployment](#deployment)
16. [Scaling to Production](#scaling-to-production)
17. [For Reviewers: What Makes This Senior-Level](#for-reviewers-what-makes-this-senior-level)

---

## Problem & Solution

### The Problem

During quarterly earnings calls, company executives make hundreds of quantitative claims about financial performance:

- *"Revenue grew 15% year-over-year"*
- *"Operating margins expanded to 28%"*
- *"Free cash flow was $4.2 billion"*

These claims are:
1. **Often inaccurate** â€” Stated growth of 15% when actual was 12.3%
2. **Sometimes misleading** â€” Using non-GAAP figures without disclosure, favorable rounding, cherry-picking metrics
3. **Systematically biased** â€” Consistently rounding up across quarters, emphasizing only positive metrics

**Manual verification is impractical** â€” A single earnings call has 30-50 quantitative claims. Verifying them requires cross-referencing multiple financial statements, handling unit conversions, computing derived metrics, and detecting subtle patterns across quarters.

### The Solution

An **automated verification pipeline** with 4 stages:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   INGEST     â”‚ â†’ â”‚   EXTRACT    â”‚ â†’ â”‚   VERIFY     â”‚ â†’ â”‚   ANALYZE    â”‚
â”‚              â”‚   â”‚              â”‚   â”‚              â”‚   â”‚              â”‚
â”‚ Fetch data   â”‚   â”‚ LLM extracts â”‚   â”‚ Compare vs   â”‚   â”‚ Detect cross-â”‚
â”‚ from FMP API â”‚   â”‚ structured   â”‚   â”‚ actual data  â”‚   â”‚ quarter      â”‚
â”‚ & store in   â”‚   â”‚ claims from  â”‚   â”‚ & assign     â”‚   â”‚ patterns of  â”‚
â”‚ SQLite       â”‚   â”‚ transcripts  â”‚   â”‚ verdicts     â”‚   â”‚ deception    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Output:** Trust scores (0-100), verdict breakdowns (verified/misleading/incorrect), discrepancy patterns, and a premium web UI for exploration.

### Value Proposition

- **For investors:** Quickly identify companies with questionable communication practices
- **For analysts:** Automate earnings call verification, focus on material discrepancies
- **For regulators:** Systematic detection of misleading communication patterns
- **For researchers:** Dataset of executive claim accuracy across companies and time

---

## Key Features

### 1. Multi-Tier Verdict System

Not just true/falseâ€”captures the full spectrum of accuracy:

| Verdict | Threshold | Example |
|---------|-----------|---------|
| âœ… **Verified** | Within 2% | Stated 15.1%, actual 15.0% |
| â‰ˆ **Approximately Correct** | Within 10% | Stated 15%, actual 13.8% |
| âš ï¸ **Misleading** | 10-25% off OR flagged | Stated 15%, actual 11.2% (or GAAP mismatch) |
| âŒ **Incorrect** | >25% off | Stated 15%, actual 9.8% |
| â“ **Unverifiable** | Can't verify | Segment-level claim without segment data |

### 2. Misleading Flag Detection

A claim can be **misleading even if numerically close**. We flag:

- **Favorable rounding** â€” Saying "grew 11%" when actual is 10.3%
- **GAAP/non-GAAP mismatch** â€” Using adjusted figures without disclosure
- **Segment vs total** â€” Quoting segment metrics against company-wide data
- **Systematic patterns** â€” Consistently rounding up, metric switching, increasing inaccuracy

### 3. Cross-Quarter Pattern Detection

Identifies 5 types of systematic deception:

| Pattern | Detection | Severity Scoring |
|---------|-----------|------------------|
| ğŸ”º **Consistent Rounding Up** | >70% of inexact claims round favorably | % of favorable roundings |
| ğŸ”„ **Metric Switching** | Most-emphasized metric changes 3+ times | Fixed 0.5 |
| ğŸ“‰ **Increasing Inaccuracy** | Accuracy declining over 3+ quarters | Magnitude of decline |
| ğŸ“Š **GAAP Shifting** | GAAP ratio changes >30% between quarters | Change magnitude |
| ğŸ¯ **Selective Emphasis** | >90% positive metrics in 2+ quarters | Fixed 0.6 |

### 4. Trust Score Algorithm

Transparent, defensible scoring:

```python
raw_score = (
    verified Ã— 1.0 +
    approx_correct Ã— 0.7 +
    misleading Ã— -0.3 +
    incorrect Ã— -1.0
) / verifiable_claims

trust_score = clamp((raw_score + 1) Ã— 50, 0, 100)
```

Maps raw scores from [-1, 1] to [0, 100]:
- **80-100:** Trustworthy
- **50-79:** Mixed track record
- **0-49:** Concerning

### 5. Production-Ready Architecture

Built with senior engineering principles:
- **6-layer clean architecture** with clear boundaries
- **Domain-driven design** â€” pure business logic, zero dependencies
- **Facade pattern** â€” single integration point for all clients
- **Retry logic with exponential backoff** â€” resilient to API failures
- **Versioned prompts** â€” iterate on LLM extraction without redeployment
- **Comprehensive testing** â€” 80+ unit tests, integration tests, E2E pipeline test
- **Multiple interfaces** â€” REST API, Streamlit UI, CLI, MCP server for AI agents

---

## Quick Start (5 Minutes)

### Prerequisites

| Requirement | Notes |
|-------------|-------|
| **Python 3.12+** | Tested on 3.12 and 3.13 |
| **FMP API Key** | Free tier at [financialmodelingprep.com](https://financialmodelingprep.com/developer/docs/) |
| **Anthropic API Key** | Get at [console.anthropic.com](https://console.anthropic.com/) |

### Setup

```bash
# 1. Navigate to backend
cd claim-auditor/backend

# 2. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure API keys
cp .env.example .env
# Edit .env and add your API keys:
#   FMP_API_KEY=your_fmp_key
#   ANTHROPIC_API_KEY=your_anthropic_key
```

### Run the Pipeline

```bash
# Run all 4 steps (ingest â†’ extract â†’ verify â†’ analyze)
python -m scripts.run_pipeline

# Expected output:
# âœ“ Ingested 10 companies, 40 transcripts, 200 financial records
# âœ“ Extracted 450 claims via Claude (~$2-5 in API costs)
# âœ“ Verified 450 claims (no API calls, pure computation)
# âœ“ Detected 18 discrepancy patterns across companies
```

### Launch the UI

```bash
streamlit run streamlit_app.py --server.port 8501

# Open http://localhost:8501
```

You'll see a premium dashboard with:
- Trust scores for all companies
- Company rankings by accuracy
- Detailed claim-by-claim verification
- Cross-quarter discrepancy patterns
- Tabbed interface for exploration

---

## Production-Ready Architecture

This system was built with **clean architecture principles** to ensure maintainability, testability, and scalability. Here's how:

### 6-Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: PRESENTATION                                       â”‚
â”‚ â€¢ FastAPI REST API  â€¢ Streamlit UI  â€¢ CLI  â€¢ MCP Server     â”‚
â”‚ Responsibility: Handle requests, render responses           â”‚
â”‚ Dependencies: Only imports PipelineFacade                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: FACADE (Integration Layer)                         â”‚
â”‚ â€¢ PipelineFacade: Single entry point for all clients        â”‚
â”‚ â€¢ Wires up services, repos, engines, clients                â”‚
â”‚ â€¢ Returns only plain dicts (never ORM models)               â”‚
â”‚ Responsibility: Hide complexity, prevent coupling           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: SERVICE (Orchestration Layer)                      â”‚
â”‚ â€¢ IngestionService   â€¢ ExtractionService                    â”‚
â”‚ â€¢ VerificationService â€¢ AnalysisService                     â”‚
â”‚ Responsibility: Coordinate workflows, handle transactions   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 4: DOMAIN (Business Logic Layer)                      â”‚
â”‚ Engines: ClaimExtractor, VerificationEngine,                â”‚
â”‚          DiscrepancyAnalyzer, MetricMapper                  â”‚
â”‚ Pure Functions: app/domain/{metrics, verdicts, scoring}.py  â”‚
â”‚ Responsibility: Core algorithms, zero external dependencies â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 5: REPOSITORY (Data Access Layer)                     â”‚
â”‚ â€¢ BaseRepository (template method pattern)                  â”‚
â”‚ â€¢ CompanyRepo, ClaimRepo, VerificationRepo, etc.            â”‚
â”‚ Responsibility: Abstract database operations                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 6: INFRASTRUCTURE                                     â”‚
â”‚ â€¢ SQLAlchemy ORM models  â€¢ HTTP clients (FMP, Anthropic)    â”‚
â”‚ â€¢ Database engine        â€¢ Configuration (Pydantic)         â”‚
â”‚ Responsibility: External systems, persistence               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Matters

**Testability:** Each layer can be tested independently. Domain logic has zero dependenciesâ€”pure functions only.

**Maintainability:** Changes to the database (swap SQLite â†’ Postgres) only affect Layer 6. Changes to UI framework only affect Layer 1.

**Scalability:** Want to add a GraphQL API? Just add a new Layer 1 consumer that uses the facade. Core logic untouched.

**Decoupling:** The facade prevents clients from directly accessing repositories or engines. Internal refactoring is safe.

### Key Design Patterns

| Pattern | Where Used | Why |
|---------|------------|-----|
| **Facade** | `PipelineFacade` | Single integration point, hides complexity |
| **Repository** | `BaseRepository` + domain repos | Data access abstraction |
| **Service Layer** | All `*Service` classes | Orchestration separate from business logic |
| **Domain-Driven Design** | `app/domain/` package | Pure business rules, zero coupling |
| **Decorator** | `@with_retry` | Resilience via exponential backoff |
| **Strategy** | Verification dispatch by metric type | Polymorphic behavior |
| **Template Method** | `BaseRepository` | Shared CRUD with overridable methods |

### Production Readiness Features

**1. Retry Logic with Exponential Backoff**
- `app/utils/retry.py` decorator
- Retries 5xx and 429 errors, fails fast on 4xx
- Jitter prevents thundering herd
- Configurable via environment variables

**2. Versioned Prompt Management**
- `app/prompts/templates/` directory structure
- Prompts loaded from files with version metadata
- Enables A/B testing, rollback, iteration without redeployment
- LRU caching for performance

**3. Domain Logic Consolidation**
- `app/domain/metrics.py` â€” Single source of truth for metric definitions and aliases
- `app/domain/verdicts.py` â€” Pure verdict assignment algorithm
- `app/domain/scoring.py` â€” All scoring formulas (accuracy, trust)
- Zero dependencies, 100% unit testable

**4. Comprehensive Testing**
- 80+ unit tests (in-memory DB, mocked APIs, <1s runtime)
- Integration tests (live FMP API validation)
- E2E pipeline test (full workflow)
- All domain logic has 100% branch coverage

**5. Premium UI Design**
- Custom CSS design system (professional color palette, typography)
- Removed prototype aesthetics (90% fewer emojis)
- Persistent sidebar navigation
- Tabbed interface (Overview / Claims / Discrepancies / Timeline)
- Trust score visualizations with gradient progress bars
- Zero information duplication

---

## Project Structure

```
claim-auditor/
â”œâ”€â”€ ARCHITECTURE_GUIDE.md          â† Deep dive for interview prep
â”œâ”€â”€ UI_REDESIGN_SPEC.md            â† UI design specification
â”œâ”€â”€ README.md                      â† You are here
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ .env                       â† API keys (git-ignored)
â”‚   â”œâ”€â”€ requirements.txt           â† Python dependencies
â”‚   â”œâ”€â”€ pyproject.toml             â† Project metadata + pytest config
â”‚   â”‚
â”‚   â”œâ”€â”€ app/                       â† Core application
â”‚   â”‚   â”œâ”€â”€ config.py              â† Settings (Pydantic + .env)
â”‚   â”‚   â”œâ”€â”€ database.py            â† SQLAlchemy setup
â”‚   â”‚   â”œâ”€â”€ facade.py              â† **PipelineFacade** (single entry point)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ clients/               â† External API clients
â”‚   â”‚   â”‚   â”œâ”€â”€ base_client.py     â† HTTP client with retry + caching
â”‚   â”‚   â”‚   â”œâ”€â”€ fmp_client.py      â† Financial Modeling Prep
â”‚   â”‚   â”‚   â””â”€â”€ llm_client.py      â† Anthropic Claude
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ models/                â† SQLAlchemy ORM (Layer 6)
â”‚   â”‚   â”‚   â”œâ”€â”€ company.py
â”‚   â”‚   â”‚   â”œâ”€â”€ transcript.py
â”‚   â”‚   â”‚   â”œâ”€â”€ financial_data.py
â”‚   â”‚   â”‚   â”œâ”€â”€ claim.py
â”‚   â”‚   â”‚   â”œâ”€â”€ verification.py
â”‚   â”‚   â”‚   â””â”€â”€ discrepancy_pattern.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ repositories/          â† Data access (Layer 5)
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py            â† Generic CRUD operations
â”‚   â”‚   â”‚   â”œâ”€â”€ company_repo.py
â”‚   â”‚   â”‚   â”œâ”€â”€ transcript_repo.py
â”‚   â”‚   â”‚   â”œâ”€â”€ financial_data_repo.py
â”‚   â”‚   â”‚   â”œâ”€â”€ claim_repo.py
â”‚   â”‚   â”‚   â”œâ”€â”€ verification_repo.py
â”‚   â”‚   â”‚   â””â”€â”€ discrepancy_pattern_repo.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ schemas/               â† Pydantic validation
â”‚   â”‚   â”‚   â”œâ”€â”€ claim.py
â”‚   â”‚   â”‚   â”œâ”€â”€ verification.py
â”‚   â”‚   â”‚   â””â”€â”€ discrepancy.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ domain/                â† **Pure business logic (Layer 4)**
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics.py         â† Metric definitions & aliases
â”‚   â”‚   â”‚   â”œâ”€â”€ verdicts.py        â† Verdict assignment algorithm
â”‚   â”‚   â”‚   â””â”€â”€ scoring.py         â† Accuracy & trust scoring
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ engines/               â† Business logic engines (Layer 4)
â”‚   â”‚   â”‚   â”œâ”€â”€ claim_extractor.py â† LLM prompt + parsing
â”‚   â”‚   â”‚   â”œâ”€â”€ metric_mapper.py   â† Metric â†’ DB column mapping
â”‚   â”‚   â”‚   â”œâ”€â”€ verification_engine.py â† **Core algorithm (400+ lines)**
â”‚   â”‚   â”‚   â””â”€â”€ discrepancy_analyzer.py â† Pattern detection
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ services/              â† Orchestration (Layer 3)
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ extraction_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ verification_service.py
â”‚   â”‚   â”‚   â””â”€â”€ analysis_service.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ prompts/               â† **Versioned LLM prompts**
â”‚   â”‚   â”‚   â”œâ”€â”€ manager.py         â† PromptManager (load + cache)
â”‚   â”‚   â”‚   â””â”€â”€ templates/
â”‚   â”‚   â”‚       â””â”€â”€ claim_extraction/
â”‚   â”‚   â”‚           â”œâ”€â”€ v1.txt     â† Extraction system prompt
â”‚   â”‚   â”‚           â””â”€â”€ metadata.json
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ retry.py           â† **Exponential backoff decorator**
â”‚   â”‚       â”œâ”€â”€ financial_math.py  â† Growth rates, margins, normalization
â”‚   â”‚       â””â”€â”€ scoring.py         â† Trust score computation
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ run_pipeline.py        â† CLI entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ claim_auditor.db       â† SQLite database (generated)
â”‚   â”‚   â”œâ”€â”€ fmp_cache/             â† Cached API responses
â”‚   â”‚   â””â”€â”€ transcripts/           â† Local transcript fallback
â”‚   â”‚       â”œâ”€â”€ AAPL_Q1_2026.txt
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ streamlit_app.py           â† **Web UI (Layer 1)**
â”‚   â”œâ”€â”€ mcp_server.py              â† AI agent interface (Layer 1)
â”‚   â”‚
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ conftest.py            â† Shared fixtures (in-memory DB)
â”‚       â”œâ”€â”€ fixtures/              â† Golden test data
â”‚       â”œâ”€â”€ unit/                  â† 80+ unit tests
â”‚       â”‚   â”œâ”€â”€ test_retry.py
â”‚       â”‚   â”œâ”€â”€ test_prompt_manager.py
â”‚       â”‚   â”œâ”€â”€ test_domain_metrics.py
â”‚       â”‚   â”œâ”€â”€ test_domain_verdicts.py
â”‚       â”‚   â”œâ”€â”€ test_domain_scoring.py
â”‚       â”‚   â”œâ”€â”€ test_verification_engine.py
â”‚       â”‚   â”œâ”€â”€ test_claim_extractor.py
â”‚       â”‚   â”œâ”€â”€ test_facade.py
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ integration/
â”‚           â”œâ”€â”€ test_fmp_live.py   â† Live API tests
â”‚           â””â”€â”€ test_pipeline_e2e.py
```

---

## How the Pipeline Works

The pipeline runs in 4 **idempotent** steps. Each step checks for existing data before processingâ€”safe to interrupt and resume.

### Step 1: Ingest (`IngestionService`)

**Fetches raw data from Financial Modeling Prep API.**

**Flow:**
```
FMP API â†’ FMPClient (with cache) â†’ IngestionService â†’ SQLite
                      â†“
          Disk cache (data/fmp_cache/*.json)
```

**For each target company:**
1. **Company profile** â€” Name, sector, industry
2. **Transcripts** â€” Earnings call text for target quarters
   - Falls back to local `.txt` files if FMP endpoint restricted
3. **Financial statements** â€” 5 quarters of income statement, cash flow, balance sheet
   - Merged by period for unified financial data records

**Idempotency:** Skips existing companies/transcripts/periods.

**Caching:** All FMP responses cached to disk. Delete `data/fmp_cache/` to force refresh.

**Cost:** ~10-20 FMP API calls per company (free tier sufficient).

---

### Step 2: Extract (`ExtractionService` â†’ `ClaimExtractor` â†’ `LLMClient`)

**Sends transcripts to Claude, extracts structured claims.**

**Flow:**
```
DB (transcripts) â†’ ClaimExtractor â†’ Claude API â†’ DB (claims)
                        â†“
            PromptManager (versioned prompts)
```

**For each transcript:**
1. Loads versioned system prompt from `app/prompts/templates/`
2. Sends transcript + prompt to Claude Sonnet 4
3. Claude returns JSON array of claims with metadata:
   ```json
   {
     "speaker": "Tim Cook, CEO",
     "claim_text": "Revenue grew 10.7% year over year",
     "metric": "revenue",
     "metric_type": "growth_rate",
     "stated_value": 10.7,
     "unit": "percent",
     "comparison_period": "year_over_year",
     "is_gaap": true,
     "confidence": 0.95
   }
   ```
4. Validates via Pydantic, normalizes metric names (`"total revenue"` â†’ `"revenue"`), deduplicates
5. Stores in `claims` table

**Idempotency:** Only processes transcripts with zero claims.

**Cost:** ~$0.10-0.50 per transcript in Claude API tokens.

---

### Step 3: Verify (`VerificationService` â†’ `VerificationEngine`)

**Compares each claim against actual financial data. No external API callsâ€”pure computation.**

**Flow:**
```
DB (claims + financial_data) â†’ VerificationEngine â†’ DB (verifications)
                                     â†“
                      MetricMapper (metric â†’ DB column)
                                     â†“
                    Domain Logic (accuracy_score, assign_verdict)
```

**For each unverified claim:**

1. **Resolve metric** â€” `MetricMapper` maps claim to DB column or derived calculation
   - Direct: `"revenue"` â†’ `financial_data.revenue`
   - Derived: `"gross_margin"` â†’ `(gross_profit / revenue) Ã— 100`

2. **Dispatch by metric type:**
   - **Growth rate / Change:** Fetch current + comparison quarter, compute actual growth
   - **Margin / Ratio:** Compute from current quarter data
   - **Absolute / Per-share:** Get raw value, normalize units

3. **Normalize units** â€” Ensure stated and actual are in same unit (billions, millions, raw)

4. **Score accuracy** â€” `accuracy_score(stated, actual)` from `app/domain/scoring.py`
   ```python
   accuracy = 1 - |stated - actual| / |actual|
   ```

5. **Check misleading flags:**
   - Rounding bias (stated 11%, actual 10.3%)
   - GAAP mismatch (claim says GAAP, used non-GAAP)
   - Segment claim without segment data

6. **Assign verdict** â€” `assign_verdict(score, flags)` from `app/domain/verdicts.py`
   - Base verdict from accuracy thresholds (2%, 10%, 25%)
   - Upgraded to "misleading" if substantive flags present

7. **Generate explanation** â€” Human-readable summary

**Idempotency:** Only processes claims without verifications.

**Cost:** Freeâ€”no API calls.

---

### Step 4: Analyze (`AnalysisService` â†’ `DiscrepancyAnalyzer`)

**Detects cross-quarter patterns of misleading communication.**

**Flow:**
```
DB (all claims + verifications) â†’ DiscrepancyAnalyzer â†’ DB (discrepancy_patterns)
```

**Pattern Detection:**

Analyzes all claims for a company across quarters to detect:

1. **Consistent Rounding Up** â€” >70% of inexact claims round favorably
2. **Metric Switching** â€” Most-emphasized metric changes 3+ quarters
3. **Increasing Inaccuracy** â€” Average accuracy declining over time
4. **GAAP Shifting** â€” GAAP ratio fluctuates >30% between quarters
5. **Selective Emphasis** â€” >90% positive metrics highlighted

**Idempotency:** Deletes old patterns before writing new ones.

**Cost:** Freeâ€”no API calls.

---

## Running the Pipeline

All commands run from `backend/` with virtual environment activated:

```bash
cd backend
source .venv/bin/activate
```

### Run All Steps

```bash
python -m scripts.run_pipeline

# Example output:
# âœ“ INGEST: 10 companies, 40 transcripts, 200 financial records (3.2s)
# âœ“ EXTRACT: 450 claims extracted via Claude (45.2s, ~52K input tokens)
# âœ“ VERIFY: 450 claims verified (0.3s)
# âœ“ ANALYZE: 18 patterns detected across 10 companies (0.1s)
# âœ“ PIPELINE COMPLETE in 49.1s
```

### Run Individual Steps

```bash
# Step 1 only: Ingest data (costs FMP API calls)
python -m scripts.run_pipeline --step ingest

# Step 2 only: Extract claims (costs Claude API tokens)
python -m scripts.run_pipeline --step extract

# Step 3 only: Verify claims (free, no API calls)
python -m scripts.run_pipeline --step verify

# Step 4 only: Analyze patterns (free, no API calls)
python -m scripts.run_pipeline --step analyze
```

### Target Specific Companies

```bash
# Only process Apple
python -m scripts.run_pipeline --tickers AAPL

# Only extract claims for Apple and Microsoft
python -m scripts.run_pipeline --step extract --tickers AAPL MSFT
```

### Adding a New Company

**Option A: One-off**
```bash
python -m scripts.run_pipeline --tickers TSLA
```

**Option B: Add to defaults**
Edit `app/config.py`:
```python
target_tickers: list[str] = [
    "AAPL", "MSFT", "NVDA", "AMZN", "GOOG",
    "META", "JPM", "JNJ", "TSLA", "CRM",
    "NEW_TICKER",  # â† Add here
]
```

Then run:
```bash
python -m scripts.run_pipeline
```

**Option C: With local transcripts**
If FMP transcript endpoint is restricted (free tier):
1. Create `data/transcripts/TSLA_Q4_2025.txt`
2. Run pipelineâ€”it automatically falls back to local files

---

## Viewing Results (Streamlit UI)

### Launch

```bash
streamlit run streamlit_app.py --server.port 8501
# Open http://localhost:8501
```

### UI Features

**Premium Design:**
- Professional color palette (navy blue primary, semantic verdict colors)
- Custom CSS overriding Streamlit defaults
- Clean typography, proper spacing (8px grid system)
- Subtle shadows and hover effects
- 90% fewer emojis (replaced with styled badges)

**Navigation:**
- **Persistent sidebar** with company list sorted by trust score
- Click any company to navigate to detail view
- Dashboard for overview

**Dashboard View:**
- Hero metrics (total claims, verified count, red flags)
- Company rankings table (sortable)
- Trust score progress bars
- Quick navigation to details

**Company Detail View (Tabbed):**

1. **Overview Tab**
   - Trust score visualization
   - Quarter-by-quarter trend
   - Discrepancy patterns with severity
   - Top 3 red flags

2. **All Claims Tab**
   - Filterable table of every claim
   - Columns: Quarter | Claim Text | Metric | Verdict | Accuracy
   - Expandable rows for full details

3. **Discrepancies Tab**
   - Only misleading/incorrect claims
   - Sorted by severity (accuracy score)
   - Full context inline

4. **Timeline Tab**
   - Quarter-by-quarter breakdown
   - Verdict distribution per quarter
   - Trend indicators (â†‘ improving, â†“ degrading, â†’ stable)

---

## Running Tests

### Unit Tests (Fast, No API Keys)

```bash
cd backend
source .venv/bin/activate

# Run all unit tests
python -m pytest tests/unit/ -v

# Expected output:
# âœ“ 80+ tests passed in <1 second
```

**Test Coverage:**

| Module | Test File | Tests | What It Tests |
|--------|-----------|-------|---------------|
| Retry Logic | `test_retry.py` | 7 | Exponential backoff, jitter, error handling |
| Prompt Manager | `test_prompt_manager.py` | 8 | Version loading, caching, latest version |
| Domain: Metrics | `test_domain_metrics.py` | 15 | Alias normalization, metric definitions |
| Domain: Verdicts | `test_domain_verdicts.py` | 12 | Verdict assignment, flag upgrades |
| Domain: Scoring | `test_domain_scoring.py` | 10 | Accuracy score, trust score formulas |
| Verification Engine | `test_verification_engine.py` | 14 | Growth/margin/absolute verification, flags |
| Claim Extractor | `test_claim_extractor.py` | 8 | Alias normalization, dedup, parsing |
| Facade | `test_facade.py` | 6 | Integration, decoupling, context manager |

**All unit tests:**
- Use **in-memory SQLite** (no file I/O)
- **Mock external clients** (no network calls)
- Run in **<1 second** total
- **Zero API keys required**

### Integration Tests (Requires API Keys)

```bash
# Live FMP API validation
python -m pytest tests/integration/test_fmp_live.py -v -m integration

# End-to-end pipeline test
python -m pytest tests/integration/test_pipeline_e2e.py -v -m integration

# Run ALL tests (unit + integration)
python -m pytest tests/ -v -m ""
```

### Test Coverage Report

```bash
python -m pytest tests/unit/ -v --cov=app --cov-report=term-missing
```

---

## Key Architectural Decisions

### 1. Clean 6-Layer Architecture

**Why:** Separation of concerns, testability, maintainability.

Each layer has a single responsibility and depends only on lower layers. Domain logic (Layer 4) has zero external dependenciesâ€”making it 100% unit testable with no mocking.

**Trade-off:** More files, more indirection. But changes to the database don't ripple to services, and changes to the UI don't touch business logic.

---

### 2. Facade Pattern for Decoupling

**Why:** External clients (UI, CLI, MCP server) never touch repos, engines, or models directly.

The `PipelineFacade` is the **only** public API. It returns plain dicts (not ORM models), preventing ORM leakage. Internal refactoring is safeâ€”only the facade interface needs stability.

**Trade-off:** One more layer of indirection. But it's worth itâ€”the Streamlit UI has zero imports of `models/`, `repositories/`, or `engines/`.

---

### 3. Domain Package for Pure Business Logic

**Why:** Business rules should have zero dependencies.

Created `app/domain/` with:
- `metrics.py` â€” Metric definitions and aliases (single source of truth)
- `verdicts.py` â€” Verdict assignment algorithm (pure function)
- `scoring.py` â€” Accuracy and trust score formulas (pure functions)

**Benefit:**
- 100% testable without database, HTTP clients, or services
- Business rules clearly documented
- Easy to change thresholds or formulas

**Trade-off:** More files. But the benefit in testability is enormous.

---

### 4. Retry Logic with Exponential Backoff

**Why:** Production systems must be resilient to transient failures.

Created `app/utils/retry.py` with:
- Exponential backoff with jitter
- Smart error handling (retry 5xx/429, fail fast on 4xx)
- Configurable via environment variables

Applied to:
- FMP API calls
- Anthropic API calls

**Trade-off:** Adds complexity. But it's encapsulated in a decorator and prevents cascading failures.

---

### 5. Versioned Prompt Management

**Why:** LLM prompts need iteration. Hardcoded prompts prevent experimentation.

Created `app/prompts/` with:
- File-based prompt templates (`v1.txt`, `v2.txt`, etc.)
- Metadata tracking (author, date, description)
- Version loading via `PromptManager`
- LRU caching for performance

**Benefit:**
- A/B test prompts without redeployment
- Rollback to previous version instantly
- Audit trail of prompt changes

**Trade-off:** More files. But enables rapid iteration on extraction quality.

---

### 6. SQLite for Development, Postgres-Ready for Production

**Why:** Intentional simplicity for demo scale.

For 10 companies Ã— 4 quarters Ã— ~10 claims each = ~400 records, SQLite is perfect:
- Zero configuration
- Single file database
- Easy to deploy
- Fast for demo workloads

But the repository pattern makes swapping to Postgres trivialâ€”just change the connection string.

**Trade-off:** SQLite has locking issues under high concurrency. But for a demo/portfolio project, it's the right choice.

---

### 7. Pre-Computed Results, Not Real-Time

**Why:** Fast UI, no waiting.

The pipeline runs once (via CLI), pre-computes all verifications and patterns. The UI just reads results. No LLM calls during exploration, no waiting.

**Trade-off:** Can't verify new companies without running the pipeline. But for a demo, instant exploration is more important.

**Production path:** Add async job queue (Celery) for background processing, REST API for triggering pipeline runs.

---

### 8. Multi-Tier Verdict System (Not Binary)

**Why:** Reality isn't binaryâ€”claims exist on a spectrum.

Not just "true" or "false":
- **Verified** (within 2%)
- **Approximately Correct** (within 10%)
- **Misleading** (10-25% or flagged)
- **Incorrect** (>25%)
- **Unverifiable** (can't check)

**Benefit:** More nuanced, defensible. Captures the real-world ambiguity of "close enough" vs. "materially wrong."

---

### 9. Misleading Detection Beyond Numbers

**Why:** Executives can be technically accurate but intentionally deceptive.

Not just accuracy scoringâ€”also flag:
- Favorable rounding
- GAAP/non-GAAP mixing
- Segment vs. total
- Cross-quarter patterns (metric switching, selective emphasis)

**Benefit:** Catches systematic deception that numeric comparison alone would miss.

---

### 10. File-Based API Caching

**Why:** Development speed and cost savings.

All FMP responses cached to `data/fmp_cache/` as JSON. Subsequent runs use cacheâ€”no redundant API calls.

**Benefit:**
- Saves API quota
- Faster development iteration
- Enables offline testing

**Trade-off:** Stale data. But cache is easily deleted to force refresh.

---

### 11. Local Transcript Fallback

**Why:** Free FMP tier restricts transcript endpoint.

If FMP returns no transcript, fall back to local `.txt` files in `data/transcripts/`. File naming: `{TICKER}_Q{quarter}_{year}.txt`.

**Benefit:** Fully functional on free FMP plan.

**Trade-off:** Manual transcript sourcing. But it makes the project accessible.

---

### 12. Comprehensive Testing (80+ Tests)

**Why:** Confidence in refactoring and changes.

Built 80+ unit tests covering:
- All domain logic (metrics, verdicts, scoring)
- Verification algorithm (growth, margin, absolute)
- Retry logic (exponential backoff, jitter, error handling)
- Prompt management (versioning, caching)
- Facade integration (decoupling, context manager)

All tests run in <1 second with no API keys.

**Trade-off:** Time investment. But it pays offâ€”refactoring is safe, bugs are caught early.

---

## API Clients & External Dependencies

### Financial Modeling Prep (FMP) API

**Base URL:** `https://financialmodelingprep.com/stable`

**Endpoints Used:**

| Endpoint | Purpose | Free Tier? |
|----------|---------|-----------|
| `/profile?symbol=AAPL` | Company name, sector | âœ… Yes |
| `/earning_call_transcript?symbol=AAPL&quarter=1&year=2026` | Transcript text | âŒ Limited (use local fallback) |
| `/income-statement?symbol=AAPL&period=quarter&limit=5` | Revenue, expenses, EPS | âœ… Yes |
| `/cash-flow-statement?...` | Operating CF, CapEx, FCF | âœ… Yes |
| `/balance-sheet-statement?...` | Assets, liabilities | âœ… Yes |

**Client:** `app/clients/fmp_client.py`
- Inherits from `BaseHTTPClient` (retry + caching)
- All responses cached to `data/fmp_cache/`
- Returns `None` or `[]` on failure (graceful degradation)

---

### Anthropic Claude API

**Model:** `claude-sonnet-4-20250514`

**Usage:** Structured claim extraction from transcripts.

**Client:** `app/clients/llm_client.py`
- Sends transcript + system prompt
- Receives JSON array of claims
- 3 fallback parsing strategies: raw JSON, markdown-fenced, regex
- Token tracking (logs total input/output tokens)

**Prompt Management:**
- System prompts stored in `app/prompts/templates/claim_extraction/`
- Loaded via `PromptManager` with version control
- Currently on v1, ready to A/B test v2

---

## Verification Algorithm

The `VerificationEngine` (`app/engines/verification_engine.py`) is the **core of the system** (400+ lines).

### Algorithm Overview (7 Steps)

```
1. Metric Resolution
   â”œâ”€ Can we map this metric to financial data?
   â””â”€ No â†’ UNVERIFIABLE âœ“ Verdict assigned

2. Metric Type Dispatch
   â”œâ”€ Growth/Change â†’ Fetch current + prior quarter
   â”œâ”€ Margin/Ratio â†’ Fetch current quarter
   â””â”€ Absolute/Per-share â†’ Fetch current quarter

3. Actual Value Computation
   â”œâ”€ Growth: ((current - prior) / |prior|) Ã— 100
   â”œâ”€ Margin: (numerator / denominator) Ã— 100
   â””â”€ Absolute: normalize_to_unit(raw, claim.unit)

4. Unit Normalization
   â””â”€ Ensure stated and actual in same unit (billions, millions, raw)

5. Accuracy Scoring
   â””â”€ accuracy = 1 - |stated - actual| / |actual|

6. Misleading Flag Detection
   â”œâ”€ Rounding bias (stated 11%, actual 10.3%)
   â”œâ”€ GAAP mismatch (claim says GAAP but used non-GAAP)
   â””â”€ Segment claim (without segment data)

7. Verdict Assignment + Explanation
   â”œâ”€ Base verdict from thresholds (2%, 10%, 25%)
   â”œâ”€ Upgrade to "misleading" if substantive flags
   â””â”€ Generate human-readable explanation
```

### Example: Growth Rate Verification

**Claim:** *"Revenue grew 15% year-over-year"*

**Verification Steps:**

1. **Resolve metric:** `"revenue"` â†’ `financial_data.revenue`
2. **Dispatch:** Growth rate â†’ fetch Q1 2026 and Q1 2025 data
3. **Compute actual:**
   ```python
   current = financial_data.revenue  # Q1 2026: $90.5B
   prior = financial_data.revenue    # Q1 2025: $81.8B
   actual_growth = (current - prior) / abs(prior) * 100
   # = (90.5 - 81.8) / 81.8 * 100 = 10.64%
   ```
4. **Score accuracy:**
   ```python
   stated = 15.0
   actual = 10.64
   accuracy = 1 - abs(15.0 - 10.64) / abs(10.64) = 0.59
   ```
5. **Check flags:** No rounding bias, no GAAP issue â†’ no flags
6. **Assign verdict:** accuracy 0.59 â†’ Misleading (below 0.75 threshold)
7. **Explanation:** "Stated 15.0% growth, actual was 10.6%. This is a 41% overstatement."

**Verdict:** âš ï¸ **Misleading**

---

### Metric Mapper

**Maps claim metrics to financial data columns or derived calculations.**

**Direct mappings:**
```python
"revenue" â†’ financial_data.revenue
"net_income" â†’ financial_data.net_income
"free_cash_flow" â†’ financial_data.free_cash_flow
"eps_diluted" â†’ financial_data.eps_diluted
```

**Derived mappings:**
```python
"gross_margin" â†’ (gross_profit / revenue) Ã— 100
"operating_margin" â†’ (operating_income / revenue) Ã— 100
"net_margin" â†’ (net_income / revenue) Ã— 100
```

**Special handling:**
- `capital_expenditure` â†’ Absolute value (FMP stores negative, transcripts state positive)

---

### Trust Score Formula

**Computed per company or per quarter:**

```python
# 1. Count verdicts
verified_count = len([c for c in claims if c.verdict == "verified"])
approx_count = len([c for c in claims if c.verdict == "approximately_correct"])
misleading_count = len([c for c in claims if c.verdict == "misleading"])
incorrect_count = len([c for c in claims if c.verdict == "incorrect"])
verifiable_total = verified_count + approx_count + misleading_count + incorrect_count

# 2. Weighted raw score
raw_score = (
    verified_count * 1.0 +
    approx_count * 0.7 +
    misleading_count * -0.3 +
    incorrect_count * -1.0
) / verifiable_total

# 3. Map to 0-100 scale
trust_score = clamp((raw_score + 1) * 50, 0, 100)
```

**Why this works:**
- Raw score ranges from -1 (all incorrect) to +1 (all verified)
- Mapping `(raw + 1) Ã— 50` converts to 0-100
- Approximately correct claims still contribute positively (0.7 weight)
- Misleading claims penalized moderately (-0.3)
- Incorrect claims severely penalized (-1.0)

**Interpretation:**
- **80-100:** High trust
- **50-79:** Mixed
- **0-49:** Concerning

---

## Cross-Quarter Pattern Detection

The `DiscrepancyAnalyzer` detects **5 types of systematic deception** across quarters.

### 1. Consistent Rounding Up ğŸ”º

**Detection:** >70% of inexact claims round favorably (stated > actual).

**Example:**
- Q1: Stated 11%, actual 10.3% âœ“ Favorable
- Q2: Stated 9%, actual 8.7% âœ“ Favorable
- Q3: Stated 12%, actual 11.8% âœ“ Favorable
- Q4: Stated 8%, actual 8.1% âœ— Unfavorable

**Favorable ratio:** 3/4 = 75% â†’ Pattern detected

**Severity:** % of favorable roundings (0.75)

---

### 2. Metric Switching ğŸ”„

**Detection:** Most-emphasized metric changes 3+ times across quarters.

**Example:**
- Q1: 5 revenue claims, 2 EPS claims â†’ most = revenue
- Q2: 3 EPS claims, 1 revenue claim â†’ most = EPS
- Q3: 4 margin claims, 2 revenue claims â†’ most = margin
- Q4: 5 EPS claims, 1 margin claim â†’ most = EPS

**Unique metrics emphasized:** {revenue, EPS, margin} = 3 â†’ Pattern detected

**Severity:** Fixed 0.5

**Why it matters:** Shifting focus to whichever metric looks best suggests cherry-picking.

---

### 3. Increasing Inaccuracy ğŸ“‰

**Detection:** Average accuracy declining over 3+ quarters.

**Example:**
- Q1: avg accuracy = 0.92
- Q2: avg accuracy = 0.88
- Q3: avg accuracy = 0.83
- Q4: avg accuracy = 0.78

**Declining trend detected** â†’ Pattern

**Severity:** Magnitude of accuracy drop (0.92 - 0.78 = 0.14)

---

### 4. GAAP/Non-GAAP Shifting ğŸ“Š

**Detection:** GAAP ratio changes >30% between consecutive quarters.

**Example:**
- Q1: 80% GAAP claims
- Q2: 40% GAAP claims (dropped 50%)
- Q3: 75% GAAP claims (jumped 87.5%)

**Change >30% detected** â†’ Pattern

**Severity:** Max change magnitude (0.875)

**Why it matters:** Inconsistent GAAP usage suggests cherry-picking favorable numbers.

---

### 5. Selective Emphasis ğŸ¯

**Detection:** >90% of growth/change claims are positive in 2+ quarters.

**Example:**
- Q1: 8 growth claims, all positive (100%)
- Q2: 6 growth claims, all positive (100%)
- Q3: 7 growth claims, 6 positive (86%)
- Q4: 9 growth claims, all positive (100%)

**Quarters with >90% positive:** 3 â†’ Pattern detected

**Severity:** Fixed 0.6

**Why it matters:** Companies rarely have *only* positive metricsâ€”highlighting only gains suggests bias.

---

## MCP Server (AI Agent Integration)

The **Model Context Protocol (MCP)** server allows AI agents (Claude Code, Cursor, custom agents) to interact with the verifier programmatically.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Agent    â”‚ (Claude Code / Cursor / Custom)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ MCP Protocol (stdio or SSE)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ mcp_server.pyâ”‚ (Layer 1)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ imports only
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PipelineFacade  â”‚ (Layer 2)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key principle:** MCP server **never imports** models, repos, engines, or services. Only uses the facade. Perfect decoupling.

---

### Available Tools

| Tool | Arguments | Returns |
|------|-----------|---------|
| `list_companies()` | â€” | All companies with trust scores |
| `analyze_company(ticker)` | `"AAPL"` | Full analysis (trust, patterns, accuracy) |
| `get_claims(ticker, verdict?)` | `"AAPL", "misleading"` | Claims, optionally filtered |
| `compare_quarters(ticker)` | `"AAPL"` | Per-quarter breakdown |
| `get_discrepancy_patterns(ticker)` | `"AAPL"` | Cross-quarter patterns |
| `run_pipeline(tickers, steps)` | `["GOOG"], "all"` | Execute pipeline steps |

---

### Running the Server

```bash
# stdio transport (for Claude Code / Cursor)
python mcp_server.py

# SSE transport (for web agents)
python mcp_server.py --transport sse
```

---

### Configuring Claude Code

Add to your MCP settings (`~/.config/claude-code/mcp_config.json`):

```json
{
  "mcpServers": {
    "claim-auditor": {
      "command": "python",
      "args": ["/absolute/path/to/claim-auditor/backend/mcp_server.py"],
      "env": {
        "FMP_API_KEY": "your-key",
        "ANTHROPIC_API_KEY": "your-key"
      }
    }
  }
}
```

Then ask Claude:
> *"List all companies in the claim auditor and show me AAPL's discrepancy patterns"*

---

## Deployment

### Docker Compose (Recommended)

```bash
cd backend

# Set API keys
export FMP_API_KEY=your_key
export ANTHROPIC_API_KEY=your_key

# Start both UI and API
docker compose up --build

# Access:
# - Streamlit UI: http://localhost:8501
# - FastAPI: http://localhost:8000
```

**Services launched:**
- `streamlit` â€” Streamlit UI
- `api` â€” FastAPI server (future use)

Both share the same `data/` volume (SQLite DB + cache).

---

### Run Individual Services

```bash
# UI only
docker compose up streamlit

# API only
docker compose up api
```

---

### Cloud Deployment (Railway / Fly.io / Render)

The Docker image supports a `MODE` env var:
- `MODE=streamlit` (default) â€” Run Streamlit UI
- `MODE=api` â€” Run FastAPI server

**Example for Railway:**
1. Connect GitHub repo
2. Set env vars: `FMP_API_KEY`, `ANTHROPIC_API_KEY`, `MODE=streamlit`
3. Deploy (auto-detects Dockerfile)

---

### Run Pipeline in Docker

```bash
docker compose exec streamlit python -m scripts.run_pipeline
```

---

## Scaling to Production

### Current State (Demo/Portfolio)

| Aspect | Current Implementation |
|--------|------------------------|
| **Scale** | 10 companies, ~40 transcripts, ~450 claims |
| **Database** | SQLite (single file) |
| **Processing** | Synchronous, sequential |
| **Scheduling** | Manual CLI run |
| **UI** | Streamlit (pre-computed results) |
| **API** | FastAPI scaffolded but unused |
| **Caching** | Disk-based JSON files |
| **Auth** | None |
| **Monitoring** | Logs to stdout |
| **Cost** | ~$5-10 per run (Claude API + FMP) |

---

### Production Scale (1,000 Companies)

| Aspect | Production Solution | Why |
|--------|---------------------|-----|
| **Database** | PostgreSQL with indexes on `company_id`, `transcript_id`, `verdict` | Concurrent reads/writes, ACID guarantees |
| **Processing** | Celery + Redis queue, async workers | Parallel extraction (10-50 transcripts at once) |
| **Scheduling** | AWS EventBridge / Cron | Daily pipeline runs after market close |
| **API** | FastAPI production deployment (Gunicorn + Uvicorn) | Real-time queries, trigger on-demand verification |
| **Caching** | Redis for verification results, S3 for FMP responses | Faster lookups, distributed caching |
| **Auth** | OAuth2 (Auth0 / Cognito) + API rate limiting | Secure multi-tenant access |
| **Monitoring** | Prometheus + Grafana + Sentry | Metrics, alerting, error tracking |
| **LLM Costs** | Batch API calls (10 transcripts per request), cache extractions | Reduce costs by 50-70% |
| **Cost** | ~$700-2,900/month | See breakdown below |

---

### Cost Estimates at Scale (1,000 Companies)

| Item | Monthly Cost | Notes |
|------|--------------|-------|
| **FMP API** | $50-200 | ~3,000 API calls/month (free tier insufficient) |
| **Claude API** | $500-2,000 | ~4,000 transcripts/quarter Ã— $0.50 avg, cache extractions |
| **PostgreSQL** | $50-200 | Managed service (AWS RDS / Render) |
| **Redis** | $20-100 | Upstash / AWS ElastiCache |
| **Compute** | $100-500 | API server + Celery workers (2-4 instances) |
| **S3 Storage** | $5-20 | FMP cache + transcript storage |
| **Monitoring** | $50-100 | Sentry + Grafana Cloud |
| **Total** | **$775-3,120/month** | ~$0.78-3.12 per company per month |

---

### What Stays the Same

The **core verification algorithm** (400+ lines in `VerificationEngine`) remains unchanged. The business logic is scale-invariant.

What changes:
- **Delivery mechanism** (batch â†’ real-time)
- **Infrastructure** (SQLite â†’ Postgres, disk cache â†’ Redis)
- **Orchestration** (CLI â†’ Celery)

But the **"why"** behind the architecture (clean layers, facade pattern, domain purity) is **designed for this evolution**.

---

## For Reviewers: What Makes This Senior-Level

If you're evaluating this codebase for a founding engineer role, here's what to look for:

### 1. Architectural Thinking, Not Just Features

**Junior engineers:** Build features that work.

**Senior engineers:** Build systems that are **maintainable**, **testable**, and **evolvable**.

This codebase demonstrates:
- **6-layer clean architecture** with explicit boundaries
- **Facade pattern** to decouple clients from implementation
- **Domain-driven design** with pure business logic
- **Repository pattern** for data access abstraction
- **Service layer** for orchestration separate from business rules

**Evidence:** The UI has zero imports of `models/`, `repositories/`, or `engines/`. It only uses `PipelineFacade`. Internal refactoring is completely safe.

---

### 2. Production-Ready Patterns

**Not just a prototypeâ€”built for evolution.**

- **Retry logic with exponential backoff** (`app/utils/retry.py`)
- **Versioned prompt management** (`app/prompts/`) for LLM iteration
- **Comprehensive testing** (80+ unit tests, integration tests, E2E)
- **Domain logic consolidation** (`app/domain/`) for testability
- **Graceful degradation** (local transcript fallback, cached API responses)

**Evidence:** The refactoring went through 4 phases to make this production-ready. It wasn't an afterthoughtâ€”it was intentional.

---

### 3. Defensible Technical Decisions

**Every choice has a reason.**

- **SQLite for dev:** Intentional simplicity, zero config, easy to deploy
- **Pre-computed results:** Fast UI, no waitingâ€”right for a demo
- **Multi-tier verdicts:** Reality isn't binaryâ€”captures the spectrum
- **Misleading detection:** Goes beyond numbersâ€”flags deceptive framing
- **File-based caching:** Development speed, cost savings, offline testing

**Evidence:** This README explains the "why" behind every decision. Not just "what" or "how."

---

### 4. Comprehensive Documentation

**Not just codeâ€”knowledge artifacts.**

- **README.md** (this file) â€” Complete project documentation
- **ARCHITECTURE_GUIDE.md** â€” Deep dive for interview prep
- **UI_REDESIGN_SPEC.md** â€” Design specification
- **Inline docstrings** on all complex functions
- **Test files as documentation** (clear naming, realistic fixtures)

**Evidence:** You could onboard a new engineer with these docs alone.

---

### 5. Thought Process Transparency

**Shows how a senior engineer thinks.**

- **Problem framing** â€” Clear articulation of the problem and constraints
- **Trade-off analysis** â€” Explains pros/cons of each decision
- **Scaling considerations** â€” Knows the path from demo to production
- **Interview preparation** â€” `ARCHITECTURE_GUIDE.md` shows ability to communicate architecture

**Evidence:** This README doesn't just explain "how to run it"â€”it explains "why it's designed this way."

---

### 6. Testing as First-Class

**Tests aren't an afterthought.**

- **80+ unit tests** covering all domain logic
- **In-memory DB** for fast tests (<1s runtime)
- **Mocked external clients** (no network calls)
- **Integration tests** for live API validation
- **E2E pipeline test** for workflow validation

**Evidence:** You can refactor with confidence. Tests are fast, isolated, and comprehensive.

---

### 7. Multiple Interfaces, One Facade

**Built for extensibility.**

4 different interfaces all use the same facade:
1. **CLI** (`scripts/run_pipeline.py`)
2. **Streamlit UI** (`streamlit_app.py`)
3. **FastAPI** (`api/`)
4. **MCP Server** (`mcp_server.py`)

**Evidence:** Adding a GraphQL API? Just add a new Layer 1 consumer. Zero changes to business logic.

---

### 8. Domain-Driven Design

**Business rules are pure, testable, and explicit.**

Created `app/domain/` with:
- `metrics.py` â€” Single source of truth for metric definitions
- `verdicts.py` â€” Verdict assignment algorithm (pure function)
- `scoring.py` â€” All scoring formulas (pure functions)

**Evidence:** Domain logic has zero dependencies. You can test accuracy scoring without a database, HTTP client, or LLM.

---

### 9. Shows Product Thinking

**Not just a backend engineerâ€”thinks about UX.**

- **Premium UI redesign** with professional aesthetics
- **User flow optimization** (persistent sidebar, tabbed interface)
- **Information architecture** (no duplication, clear hierarchy)
- **Trust-building design** (clean, polished, credible)

**Evidence:** The UI went through a complete redesign to feel premium. This shows product empathy.

---

### 10. Knows the Path to Production

**Demo scale, but designed for evolution.**

The `Scaling to Production` section shows:
- Current state (SQLite, synchronous processing)
- Production state (Postgres, Celery + Redis, caching)
- Cost estimates at scale ($700-2,900/month for 1,000 companies)

**Evidence:** This isn't just a toy project. It's a foundation that can scale.

---

## Summary

**This isn't just a claim verification system. It's a demonstration of senior-level software engineering.**

Key takeaways:
- âœ… **Clean architecture** with 6 layers and explicit boundaries
- âœ… **Production-ready patterns** (retry logic, versioned prompts, comprehensive testing)
- âœ… **Domain-driven design** (pure business logic, zero coupling)
- âœ… **Multiple interfaces** (CLI, UI, API, MCP) all using the same facade
- âœ… **Defensible decisions** (every choice explained with trade-offs)
- âœ… **Comprehensive docs** (README, ARCHITECTURE_GUIDE, UI_REDESIGN_SPEC)
- âœ… **Path to production** (knows what needs to change for scale)

**For interview prep:** Read the `ARCHITECTURE_GUIDE.md` for deep dives, Q&A framework, and talking points.

**For running the code:** Follow the Quick Start aboveâ€”you'll be exploring results in 5 minutes.

**For reviewers:** This codebase shows the thought process of a founding engineer who can build, scale, and maintain systems.

---

## License

MIT

---

## Contact

For questions or feedback, please open an issue on GitHub.
